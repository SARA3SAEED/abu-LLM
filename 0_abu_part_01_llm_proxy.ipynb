{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SARA3SAEED/abu-LLM/blob/main/0_abu_part_01_llm_proxy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install + Run Ollama Server"
      ],
      "metadata": {
        "id": "CKcVrEjUBxSq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSsnLTFQMGWU"
      },
      "outputs": [],
      "source": [
        "!curl https://ollama.ai/install.sh | sh"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ollama_model_id = \"qwen2:7b-instruct-q4_K_M\""
      ],
      "metadata": {
        "id": "3p6vxZzS6qnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup ollama serve &\n",
        "!sleep 8 && tail nohup.out"
      ],
      "metadata": {
        "id": "2N9TiyEpMTQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull {ollama_model_id}"
      ],
      "metadata": {
        "id": "lEY30NLsM75l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup ollama run {ollama_model_id} &\n",
        "!sleep 8 && tail nohup.out"
      ],
      "metadata": {
        "id": "b5Nh7nNiNS0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Ollama"
      ],
      "metadata": {
        "id": "P3FqY3-eNmVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ollama==0.3.2"
      ],
      "metadata": {
        "id": "JWNspgHuOeME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ollama\n",
        "import requests\n",
        "import json\n",
        "from pprint import pprint"
      ],
      "metadata": {
        "id": "b3rqqZS1PlW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Using Python Requests"
      ],
      "metadata": {
        "id": "jTX4cq28O5xO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resp = requests.post(\"http://localhost:11434/api/generate\", json={\n",
        "    \"model\": ollama_model_id,\n",
        "    \"prompt\": \"Say Hi in French\",\n",
        "    \"stream\": False\n",
        "})\n",
        "\n",
        "if resp.status_code != 200:\n",
        "    raise Exception(\"Non-200 response: \" + str(resp.content))\n",
        "else:\n",
        "    pprint(resp.json())"
      ],
      "metadata": {
        "id": "WNzVamTMN4Tk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Using Ollama-Python"
      ],
      "metadata": {
        "id": "XOfkB5vwO9Ac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = ollama.chat(model=ollama_model_id, messages=[\n",
        "  {\n",
        "    'role': 'user',\n",
        "    'content': \"كيف يمكنني السؤال عن أقرب متجر باللغة الفرنسية؟\",\n",
        "  },\n",
        "])\n",
        "\n",
        "pprint(response['message']['content'])"
      ],
      "metadata": {
        "id": "79DRePbiO8nw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ollama\n",
        "\n",
        "stream = ollama.chat(\n",
        "    model=ollama_model_id,\n",
        "    messages=[{'role': 'user', 'content': 'اذكر خمس أنواع فواكه تحتوي على نسب عالية من فيتامين ج؟'}],\n",
        "    stream=True,\n",
        ")\n",
        "\n",
        "for chunk in stream:\n",
        "  print(chunk['message']['content'], end='', flush=True)"
      ],
      "metadata": {
        "id": "XOgC6l9nNpEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLM Proxy"
      ],
      "metadata": {
        "id": "xsG_lxD6SS4d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ check any litellm processes\n",
        "# !pgrep -fl litellm\n",
        "\n",
        "# ============ kill any litellm processes\n",
        "# !pkill -f litellm"
      ],
      "metadata": {
        "id": "W68r_LpTVrra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install 'litellm[proxy]'==1.44.9 openai==1.42.0"
      ],
      "metadata": {
        "id": "H_SEB1cdSUa1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ['GROQ_API_KEY'] = userdata.get('GROQ_API_KEY')"
      ],
      "metadata": {
        "id": "ZdU_ze2xXYuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile llm.config\n",
        "model_list:\n",
        "  - model_name: \"qwen2:7b-ollama\"\n",
        "    litellm_params:\n",
        "      model: \"ollama/qwen2:7b-instruct-q4_K_M\"\n",
        "      api_base: http://localhost:11434\n",
        "\n",
        "  - model_name: \"groq-gemma9b\"\n",
        "    litellm_params:\n",
        "      model: \"groq/gemma2-9b-it\"\n",
        "      api_key: \"os.environ/GROQ_API_KEY\"\n",
        "\n",
        "  - model_name: \"groq-mixtral\"\n",
        "    litellm_params:\n",
        "      model: \"groq/mixtral-8x7b-32768\"\n",
        "      api_key: \"os.environ/GROQ_API_KEY\""
      ],
      "metadata": {
        "id": "lKEWh2C5Sh0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup litellm --config llm.config &\n",
        "!sleep 8 && tail nohup.out"
      ],
      "metadata": {
        "id": "jRJY7mVhTEi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from pprint import pprint\n",
        "\n",
        "client = openai.OpenAI(\n",
        "    api_key=\"anything\",\n",
        "    base_url=\"http://0.0.0.0:4000\"\n",
        ")\n",
        "\n",
        "# request sent to model set on litellm proxy, `litellm --model`\n",
        "response = client.chat.completions.create(model=\"groq-gemma9b\", messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"لماذا تبدو السماء زرقاء بالنهار؟\"\n",
        "    }],\n",
        "    temperature=0.5,\n",
        "    max_tokens=512\n",
        ")\n",
        "\n",
        "if response and response.choices:\n",
        "    print(\"model:\", response.model)\n",
        "    pprint(response.choices[0].message.content)\n"
      ],
      "metadata": {
        "id": "cWKqXmjHTcjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from pprint import pprint\n",
        "\n",
        "client = openai.OpenAI(\n",
        "    api_key=\"anything\",\n",
        "    base_url=\"http://0.0.0.0:4000\"\n",
        ")\n",
        "\n",
        "# request sent to model set on litellm proxy, `litellm --model`\n",
        "response = client.chat.completions.create(model=\"groq-gemma9b\", messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"لماذا تبدو السماء زرقاء بالنهار؟\"\n",
        "    }\n",
        "])\n",
        "\n",
        "if response and response.choices:\n",
        "    print(\"model:\", response.model)\n",
        "    pprint(response.choices[0].message.content)\n"
      ],
      "metadata": {
        "id": "CB_D5usmX9nE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LiteLLM Logs"
      ],
      "metadata": {
        "id": "s-8k-MonzvgY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import litellm\n",
        "from litellm.integrations.custom_logger import CustomLogger\n",
        "from litellm import completion, acompletion\n",
        "import os\n",
        "import json\n",
        "\n",
        "logs_dir = \"./llm-logs\"\n",
        "os.makedirs(logs_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "def log_post_api_call(  kwargs,                 # kwargs to completion\n",
        "                        completion_response,    # response from completion\n",
        "                        start_time, end_time    # start/end time\n",
        "                        ):\n",
        "    with open(os.path.join(logs_dir, \"post-llm-call.jsonl\"), \"a\") as dest:\n",
        "        dest.write(json.dumps({\n",
        "            \"kwargs\": kwargs,\n",
        "            \"completion_response\": completion_response,\n",
        "            \"start_time\": start_time,\n",
        "            \"end_time\": end_time,\n",
        "        }, default=str, ensure_ascii=False) + \"\\n\" )\n",
        "\n",
        "litellm.success_callback = [log_post_api_call]\n",
        "\n",
        "response = completion(model=\"groq/gemma2-9b-it\", messages = [\n",
        "                        {\n",
        "                            \"role\": \"user\",\n",
        "                            \"content\": \"لماذا تبدو السماء زرقاء بالنهار؟\"\n",
        "                        }\n",
        "                    ], stream=False)\n",
        "\n",
        "if response and response.choices:\n",
        "    print(\"model:\", response.model)\n",
        "    pprint(response.choices[0].message.content)\n",
        "\n",
        "# =========== in case of stream\n",
        "# for chunk in response:\n",
        "#     print(chunk['choices'][0]['delta'].content, end='', flush=True)\n",
        "#     continue"
      ],
      "metadata": {
        "id": "WUc-8D4jz1j6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LiteLLM Load Balancer"
      ],
      "metadata": {
        "id": "2lVYWMcOZpRp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile llm-lb.config\n",
        "model_list:\n",
        "  - model_name: \"myapp-llm\"\n",
        "    litellm_params:\n",
        "      model: \"ollama/qwen2:7b-instruct-q4_K_M\"\n",
        "      api_base: http://localhost:11434\n",
        "      rpm: 2\n",
        "\n",
        "  - model_name: \"myapp-llm\"\n",
        "    litellm_params:\n",
        "      model: \"groq/gemma2-9b-it\"\n",
        "      api_key: \"os.environ/GROQ_API_KEY\"\n",
        "      rpm: 5\n",
        "\n",
        "  - model_name: \"myapp-llm\"\n",
        "    litellm_params:\n",
        "      model: \"groq/mixtral-8x7b-32768\"\n",
        "      api_key: \"os.environ/GROQ_API_KEY\"\n",
        "      rpm: 5\n",
        "\n",
        "routing_strategy: simple-shuffle # Literal[\"simple-shuffle\", \"least-busy\",]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbnFt2hKZs7u",
        "outputId": "f0a5c32b-f03f-4dc1-d0db-722e43ac6508"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing llm-lb.config\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup litellm --config llm-lb.config &\n",
        "!sleep 8 && tail nohup.out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtUS7qWeaaMc",
        "outputId": "7bf0a11b-1607-42e0-9a4d-b8113b96fa5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n",
            "[GIN] 2024/08/29 - 04:28:35 | 200 |   6.54177999s |       127.0.0.1 | POST     \"/api/generate\"\n",
            "INFO:     127.0.0.1:37062 - \"POST /chat/completions HTTP/1.1\" 200 OK\n",
            "INFO:     Shutting down\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:     Finished server process [9826]\n",
            "INFO:     Started server process [11726]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:4000 (Press CTRL+C to quit)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from pprint import pprint\n",
        "\n",
        "client = openai.OpenAI(\n",
        "    api_key=\"anything\",\n",
        "    base_url=\"http://0.0.0.0:4000\"\n",
        ")\n",
        "\n",
        "# request sent to model set on litellm proxy, `litellm --model`\n",
        "response = client.chat.completions.create(model=\"myapp-llm\", messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"لماذا تبدو السماء زرقاء بالنهار؟\"\n",
        "    }\n",
        "])\n",
        "\n",
        "if response and response.choices:\n",
        "    print(\"model:\", response.model)\n",
        "    pprint(response.choices[0].message.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiFSX7oJUOxF",
        "outputId": "ec9b9a2b-4f66-40f0-afc5-ac9c28b3cfe2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model: ollama/qwen2:7b-instruct-q4_K_M\n",
            "('السماء تبدو زرقاء بسبب عملية تناثر الضوء في الغلاف الجوي لل зم. عندما يدخل '\n",
            " 'الأشعة الشمسية الكرة ، تنكسر جزء من الضوء البنفسجي واليابس بشكل أكبر من '\n",
            " 'الألوان الأخرى بسبب بقائها الطويل في الغلاف الجوي. لكن الفعل الأكثر أهمية هو '\n",
            " 'قدرة الغازات مثل الأوكسجين والأوزون والنيتروجين على تنثر الضوء بكفاءة في '\n",
            " 'جميع الاتجاهات. هذا التشتت يضيف اللون الزرقاء إلى كل من الأشعة تحت الحمراء '\n",
            " 'وأجزاء أخرى من الطيف، مما يجعل السماء تبدو باللون الزرقاء لمعظم الوقت.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Expose LiteLLM Port"
      ],
      "metadata": {
        "id": "W4eQUe0eb8ea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok==7.2.0"
      ],
      "metadata": {
        "id": "nfKsmkeicAIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from pyngrok import ngrok, conf\n",
        "\n",
        "# Enter your authtoken, which can be copied from https://dashboard.ngrok.com/auth\n",
        "conf.get_default().auth_token = userdata.get('ngrok')\n",
        "\n",
        "port = \"4000\"\n",
        "\n",
        "# Open a ngrok tunnel to the HTTP server\n",
        "public_url = ngrok.connect(port).public_url\n",
        "print(public_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DeqJ_GetcFgK",
        "outputId": "da1cbdcd-260d-4699-e6e3-1155570b9271"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://aed5-34-124-211-231.ngrok-free.app\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run Colab Terminal"
      ],
      "metadata": {
        "id": "Ph6v5w16e13I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-colab-shell==0.2"
      ],
      "metadata": {
        "id": "qaA1MsQAe4TI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google_colab_shell import getshell"
      ],
      "metadata": {
        "id": "ZTWhld0BfKyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "getshell(height=600)"
      ],
      "metadata": {
        "id": "G_fH1ntVe_dn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "getshell(height=400)"
      ],
      "metadata": {
        "id": "7HVkfrO4fT9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xrgeRuHvvrz7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}