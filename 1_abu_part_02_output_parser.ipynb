{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SARA3SAEED/abu-LLM/blob/main/1_abu_part_02_output_parser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJrbVkcphg2e"
      },
      "outputs": [],
      "source": [
        "!pip install 'litellm[proxy]'==1.44.9 openai==1.42.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ['GROQ_API_KEY'] = userdata.get('GROQ_API_KEY')\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('openai-colab')\n",
        "\n",
        "# ============ kill any litellm processes\n",
        "# !pkill -f litellm"
      ],
      "metadata": {
        "id": "CskW8iXhiQu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile llm.config\n",
        "model_list:\n",
        "\n",
        "  - model_name: \"groq-gemma9b\"\n",
        "    litellm_params:\n",
        "      model: \"groq/gemma2-9b-it\"\n",
        "      api_key: \"os.environ/GROQ_API_KEY\"\n",
        "\n",
        "  - model_name: \"groq-mixtral\"\n",
        "    litellm_params:\n",
        "      model: \"groq/mixtral-8x7b-32768\"\n",
        "      api_key: \"os.environ/GROQ_API_KEY\"\n",
        "\n",
        "  - model_name: \"openai-gpt4o-mini\"\n",
        "    litellm_params:\n",
        "      model: \"openai/gpt-4o-mini\"\n",
        "      api_key: \"os.environ/OPENAI_API_KEY\""
      ],
      "metadata": {
        "id": "PK0P3CUliALD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup litellm --config llm.config &\n",
        "!sleep 15 && tail nohup.out"
      ],
      "metadata": {
        "id": "6MDIXZ67iHw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set LiteLLM Logs"
      ],
      "metadata": {
        "id": "tDfvuZ6pCgpl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import litellm\n",
        "from litellm.integrations.custom_logger import CustomLogger\n",
        "from litellm import completion, acompletion\n",
        "import os\n",
        "import json\n",
        "\n",
        "logs_dir = \"./llm-logs\"\n",
        "os.makedirs(logs_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "def log_post_api_call(  kwargs,                 # kwargs to completion\n",
        "                        completion_response,    # response from completion\n",
        "                        start_time, end_time    # start/end time\n",
        "                        ):\n",
        "    with open(os.path.join(logs_dir, \"post-llm-call.jsonl\"), \"a\") as dest:\n",
        "        dest.write(json.dumps({\n",
        "            \"kwargs\": kwargs,\n",
        "            \"completion_response\": completion_response,\n",
        "            \"start_time\": start_time,\n",
        "            \"end_time\": end_time,\n",
        "        }, default=str, ensure_ascii=False) + \"\\n\" )\n",
        "\n",
        "litellm.success_callback = [log_post_api_call]"
      ],
      "metadata": {
        "id": "AEl2EDk1CihD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Output Parser - Basics"
      ],
      "metadata": {
        "id": "lG6gDNb-iYNP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Example 1"
      ],
      "metadata": {
        "id": "-2MVX3qblx5Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import json\n",
        "from pprint import pprint\n",
        "from pydantic import BaseModel, Field\n",
        "from litellm import completion, acompletion\n",
        "from typing import List\n",
        "\n",
        "def parse_json(text):\n",
        "    text = text.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
        "    try:\n",
        "        return json.loads(text)\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "class PersonalDetails(BaseModel):\n",
        "    name: str= Field(..., description=\"The name of the person\")\n",
        "    age: int=Field(..., description=\"The age of the person\")\n",
        "    gender: str=Field(..., description=\"The gender of the person\")\n",
        "    nationality: str=Field(..., description=\"The nationality of the person\")\n",
        "    graduated_in: str=Field(..., description=\"The university or college that the person graduated in\")\n",
        "    occupation: str=Field(..., description=\"The occupation of the person\")\n",
        "    interests: str=Field(..., description=\"The interests of the person\")\n",
        "    similiar_figures_names: List[str] = Field(..., description=\"The similiar figures names of the person\")\n",
        "\n",
        "# client = openai.OpenAI(\n",
        "#     api_key=\"anything\",\n",
        "#     base_url=\"http://0.0.0.0:4000\"\n",
        "# )\n",
        "\n",
        "# response = client.chat.completions.create(model=\"groq-gemma9b\", messages = [])"
      ],
      "metadata": {
        "id": "7UfEfxF6jzSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# request sent to model set on litellm proxy, `litellm --model`\n",
        "response = completion(model=\"groq/gemma2-9b-it\", messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"\\n\".join([\n",
        "            \"You are an NLP data paraser. You have to parse natural text to specific data scheme.\",\n",
        "            \"You will be provided by a text and a pydantic scheme.\",\n",
        "            \"You have to generate a json object that matches the pydantic scheme, and filled with extracted data from text\",\n",
        "            \"Do not translate values. Fill the values as they are in the text.\",\n",
        "            \"Do not generate any introduction or conclusions. Just generate the JSON output.\",\n",
        "        ]),\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\":\"\\n\".join([\n",
        "            \"### Input Text:\",\n",
        "            \"\"\" علي مصطفى مشرفة باشا (11 يوليو 1898 – 15 يناير 1950) هو عالم فيزياء نظرية مصري. من مواليد دمياط. يُلقّب بأينشتاين العرب لأن أبحاثه كانت في نفس المجال ونفس الموضوعات التي كانت أبحاث ألبرت أينشتاين تدور حولها. تخرج في مدرسة المعلمين العليا عام 1917، وحصل على دكتوراه فلسفة العلوم من جامعة لندن عام 1923، ثم كان أول مصري يحصل على درجة دكتوراه العلوم من إنجلترا من جامعة لندن عام 1924. عُيّن أستاذاً للرياضيات في مدرسة المعلمين العليا ثم للـرياضيات التطبيقية في كلية العلوم عام 1926. مُنح لقب أستاذ من جامعة القاهرة وهو دون الثلاثين من عمره. انتُخب في عام 1936 عميدا لكلية العلوم، فأصبح بذلك أول عميد مصري لها. حصل على لقب الباشاوية من الملك فاروق. تتلمذ على يده مجموعة من أشهر علماء مصر، ومن بينهم سميرة موسى.\"\"\",\n",
        "            \"\",\n",
        "            \"### PyDantic Scheme:\",\n",
        "            PersonalDetails.schema_json(),\n",
        "            \"\",\n",
        "            \"### Output JSON:\",\n",
        "            \"```json\"\n",
        "        ])\n",
        "    }\n",
        "])\n",
        "\n",
        "if response and response.choices:\n",
        "    print(\"model:\", response.model)\n",
        "    resp_text = response.choices[0].message.content\n",
        "    pprint(parse_json(resp_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hG3QgSy4ig-K",
        "outputId": "fea60bad-2116-4d58-f7ed-acf09bb335a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model: groq/gemma2-9b-it\n",
            "{'age': None,\n",
            " 'gender': 'male',\n",
            " 'graduated_in': 'University of London',\n",
            " 'interests': 'Physics',\n",
            " 'name': 'علي مصطفى مشرفة باشا',\n",
            " 'nationality': 'Egyptian',\n",
            " 'occupation': 'Professor of Mathematics & Applied Mathematics',\n",
            " 'similiar_figures_names': ['ألبرت أينشتاين']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Example 2"
      ],
      "metadata": {
        "id": "-JGuxgzWl16E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import json\n",
        "from pprint import pprint\n",
        "from litellm import completion, acompletion\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List\n",
        "from enum import Enum\n",
        "\n",
        "def parse_json(text):\n",
        "    text = text.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
        "    try:\n",
        "        return json.loads(text)\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "# Enum for messsage intents\n",
        "class MessageIntent(str, Enum):\n",
        "    say_greeting = \"say_greeting\"\n",
        "    ask_weather_status = \"ask_weather_status\"\n",
        "    ask_light_off = \"ask_light_off\"\n",
        "    ask_light_on = \"ask_light_on\"\n",
        "    ask_light_status = \"ask_light_status\"\n",
        "    ask_not_specified = \"ask_not_specified\"\n",
        "\n",
        "class MessageSentiment(str, Enum):\n",
        "    positive_sentiment = \"positive_sentiment\"\n",
        "    negative_sentiment = \"negative_sentiment\"\n",
        "    no_detected_sentiment = \"no_detected_sentiment\"\n",
        "\n",
        "class Message(BaseModel):\n",
        "    intents: List[MessageIntent] = Field(..., description=\"The set of detected intents of the message\", min_items=1, max_items=3)\n",
        "    sentiment: MessageSentiment = Field(..., description=\"The sentiment of the message\")\n"
      ],
      "metadata": {
        "id": "zRbqHbv3l3WA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = completion(model=\"groq/gemma2-9b-it\", messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"\\n\".join([\n",
        "            \"You are an NLP data paraser. You have to parse natural text to specific data scheme.\",\n",
        "            \"You will be provided by a text and a pydantic scheme.\",\n",
        "            \"You have to generate a json object that matches the pydantic scheme, and filled with extracted data from text\",\n",
        "            \"Do not translate values. Fill the values as they are in the text.\",\n",
        "            \"Do not generate any introduction or conclusions. Just generate the JSON output.\",\n",
        "        ]),\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\":\"\\n\".join([\n",
        "            \"### Input Text:\",\n",
        "            \"كم درجة الحرارة خارج الغرفة حاليا ؟ و يا ريت تضوي نور الغرفة\",\n",
        "            # \"ولعلي نور الصالة. فاهم يا غبي, الصالة مش الأوضة\"\n",
        "            \"\",\n",
        "            \"### PyDantic Scheme:\",\n",
        "            Message.schema_json(),\n",
        "            \"\",\n",
        "            \"### Output JSON:\",\n",
        "            \"```json\"\n",
        "        ])\n",
        "    }\n",
        "])\n",
        "\n",
        "if response and response.choices:\n",
        "    print(\"model:\", response.model)\n",
        "    resp_text = response.choices[0].message.content\n",
        "    resp_json = parse_json(resp_text)\n",
        "    if resp_json:\n",
        "        filled_scheme = Message(**resp_json)\n",
        "        pprint(filled_scheme.dict())"
      ],
      "metadata": {
        "id": "wWikvp4gm3Gj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filled_scheme.intents[0].value"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "aY96iJ09nlBL",
        "outputId": "33daebae-1057-4284-f4b9-6a3d39463875"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ask_weather_status'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Output Parser using LangChain"
      ],
      "metadata": {
        "id": "4G0AR5EFGH2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain==0.1.20"
      ],
      "metadata": {
        "id": "yDWJ36nEGKSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.chat_models import ChatLiteLLM\n",
        "from langchain_core.messages import HumanMessage"
      ],
      "metadata": {
        "id": "_W2uQQ6NGgz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat = ChatLiteLLM(model=\"groq/gemma2-9b-it\", temperature=0.5)"
      ],
      "metadata": {
        "id": "IAbPArIiGpMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's test the chat module\n",
        "messages = [\n",
        "    HumanMessage(\n",
        "        content=\"كيف يسطع القمر بالنور ؟\"\n",
        "    )\n",
        "]\n",
        "chat(messages)"
      ],
      "metadata": {
        "id": "GficK0OJGt8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pprint import pprint\n",
        "from litellm import completion, acompletion\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List\n",
        "from enum import Enum\n",
        "\n",
        "# Enum for messsage intents\n",
        "class MessageIntent(str, Enum):\n",
        "    say_greeting = \"say_greeting\"\n",
        "    ask_weather_status = \"ask_weather_status\"\n",
        "    ask_light_off = \"ask_light_off\"\n",
        "    ask_light_on = \"ask_light_on\"\n",
        "    ask_light_status = \"ask_light_status\"\n",
        "    ask_not_specified = \"ask_not_specified\"\n",
        "\n",
        "class MessageSentiment(str, Enum):\n",
        "    message_with_positive_sentiment = \"message_with_positive_sentiment\"\n",
        "    message_with_negative_sentiment = \"message_with_negative_sentiment\"\n",
        "    message_with_neutral_sentiment = \"message_with_neutral_sentiment\"\n",
        "\n",
        "class Message(BaseModel):\n",
        "    intents: List[MessageIntent] = Field(..., description=\"The set of detected intents of the message\", min_items=1, max_items=3)\n",
        "    sentiment: MessageSentiment = Field(..., description=\"The sentiment of the message\")"
      ],
      "metadata": {
        "id": "rXNpGGLRIjlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field, validator"
      ],
      "metadata": {
        "id": "y88ODcPnIsSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up a parser + inject instructions into the prompt template.\n",
        "parser = PydanticOutputParser(pydantic_object=Message)\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=\"أجب عن سؤال المستخدم.\\n{format_instructions}\\n{query}\\n\",\n",
        "    input_variables=[\"query\"],\n",
        "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
        ")\n",
        "\n",
        "chain = prompt | chat | parser\n",
        "\n",
        "chain.invoke({\"query\":  \"كم درجة الحرارة خارج الغرفة حاليا ؟ و يا ريت تضوي نور الغرفة\",})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-3JR4XRIxTH",
        "outputId": "6f8daae4-01b1-462b-8a47-c778a3b533c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Message(intents=[<MessageIntent.ask_light_on: 'ask_light_on'>, <MessageIntent.ask_weather_status: 'ask_weather_status'>], sentiment=<MessageSentiment.message_with_positive_sentiment: 'message_with_positive_sentiment'>)"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Synthetic Data Generation"
      ],
      "metadata": {
        "id": "_gq7xXxlLUtZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Example 1"
      ],
      "metadata": {
        "id": "3amd4U-0O1A-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pprint import pprint\n",
        "from litellm import completion, acompletion\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List\n",
        "from enum import Enum\n",
        "\n",
        "# class Receipe(BaseModel):\n",
        "#     receipe_name: str = Field(..., description=\"The name of the receipe\")\n",
        "#     ingredients: List[str] = Field(..., description=\"The ingredients of the receipe\", min_items=1, max_items=20)\n",
        "#     cooking_steps: List[str] = Field(..., description=\"The cooking steps of the receipe\", min_items=1, max_items=20)\n",
        "#     cook_time: int = Field(..., description=\"The cooking time of the receipe in minutes\")\n",
        "#     serving_instructions : List[str] = Field(..., description=\"The serving instructions of the receipe\", min_items=1, max_items=20)\n",
        "\n",
        "class Receipe(BaseModel):\n",
        "    receipe_name: str = Field(..., description=\"اسم الوصفة\")\n",
        "    ingredients: List[str] = Field(..., description=\"مكونات الوصفة\", min_items=1, max_items=10)\n",
        "    cooking_steps: List[str] = Field(..., description=\"خطوات طهو الوصفة.\", min_items=1, max_items=10)\n",
        "    cook_time: int = Field(..., description=\"عدد الدقائق اللازمة لطهو الوصفة.\")\n",
        "    serving_instructions : List[str] = Field(..., description=\"إرشادات لتقديم الوصفة بعد الطهو.\", min_items=1, max_items=5)\n"
      ],
      "metadata": {
        "id": "hJ7wERX9LZOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = completion(model=\"groq/gemma2-9b-it\", messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"\\n\".join([\n",
        "            \"You are an NLP data generator. You have to generate a popular reciepe in Middle East.\",\n",
        "            \"You will be provided by a receipe name, you have to generate the receipe details in Arabic.\",\n",
        "            \"The receipe details must be in Arabic. Avoid English terms.\"\n",
        "            \"You have to generate a json object that matches the pydantic scheme, and filled with extracted data from text\",\n",
        "            \"Do not generate any introduction or conclusions. Just generate the JSON output.\",\n",
        "        ]),\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\":\"\\n\".join([\n",
        "            \"### Receipe Name:\",\n",
        "            \"دولما عراقي\",\n",
        "            \"\",\n",
        "            \"### PyDantic Scheme:\",\n",
        "            Receipe.schema_json(),\n",
        "            \"\",\n",
        "            \"### Receipe Details in JSON:\",\n",
        "            \"```json\"\n",
        "        ])\n",
        "    }\n",
        "], temperature=0.5, max_tokens=1024)\n",
        "\n",
        "if response and response.choices:\n",
        "    print(\"model:\", response.model)\n",
        "    resp_text = response.choices[0].message.content\n",
        "    resp_json = parse_json(resp_text)\n",
        "    if resp_json:\n",
        "        filled_scheme = Receipe(**resp_json)\n",
        "        pprint(filled_scheme.dict())"
      ],
      "metadata": {
        "id": "zlFRlEFAMK09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Example 2"
      ],
      "metadata": {
        "id": "lzS96MZdOz9O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Receipe(BaseModel):\n",
        "    receipe_name: str = Field(..., description=\"The name of the receipe\")\n",
        "    ingredients: List[str] = Field(..., description=\"The ingredients of the receipe\", min_items=1, max_items=20)\n",
        "    cooking_steps: List[str] = Field(..., description=\"The cooking steps of the receipe\", min_items=1, max_items=20)\n",
        "    cook_time: int = Field(..., description=\"The cooking time of the receipe in minutes\")\n",
        "    serving_instructions : List[str] = Field(..., description=\"The serving instructions of the receipe\", min_items=1, max_items=20)\n",
        "\n",
        "class GeneratedPrompt(BaseModel):\n",
        "    system_message: str = Field(..., description=\"The system message to guide the language model about its rule.\")\n",
        "    user_message: str = Field(..., description=\"The user message to ask the language model for the receipe\")\n",
        "    generated_receipe: Receipe = Field(..., description=\"The generated receipe\")"
      ],
      "metadata": {
        "id": "16N7Ko3JO6ET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = completion(model=\"groq/gemma2-9b-it\", messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"\\n\".join([\n",
        "            \"You are an NLP data generator. You have to generate training data to finetune a language model on generating popular reciepes in Middle East.\",\n",
        "            \"You will be provided by a receipe name, you have to generate the full training sample including system message, user message, and the generated model response for the receipe details in Arabic.\",\n",
        "            \"You have to generate a json object that matches the pydantic scheme, and filled with extracted data from text\",\n",
        "            \"Do not generate any introduction or conclusions. Just generate the JSON output.\",\n",
        "        ]),\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\":\"\\n\".join([\n",
        "            \"### Receipe Name:\",\n",
        "            \"دولما عراقي\",\n",
        "            \"\",\n",
        "            \"### PyDantic Scheme:\",\n",
        "            GeneratedPrompt.schema_json(),\n",
        "            \"\",\n",
        "            \"### LLM Training Sample:\",\n",
        "            \"```json\"\n",
        "        ])\n",
        "    }\n",
        "], temperature=0.5, max_tokens=1024)\n",
        "\n",
        "if response and response.choices:\n",
        "    print(\"model:\", response.model)\n",
        "    resp_text = response.choices[0].message.content\n",
        "    resp_json = parse_json(resp_text)\n",
        "    if resp_json:\n",
        "        filled_scheme = GeneratedPrompt(**resp_json)\n",
        "        pprint(filled_scheme.dict())"
      ],
      "metadata": {
        "id": "ho4hUAv_O8Jr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DIY**\n",
        "\n",
        "### Task:\n",
        "\n",
        "Develop an LLM Assistant to generate valid data to finetune an LLM on a specific task.\n",
        "\n",
        "Choose one of these tasks:\n",
        "\n",
        "    - Assisting a customer support center in detecting the sentiment of user messages and identifying which products are mentioned.\n",
        "\n",
        "    - Creating training data for an LLM that can identify and correct grammatical errors in sentences.\n",
        "\n",
        "    - Developing a dataset for training an LLM to recognize and extract entities from medical texts, including diseases, medications, and symptoms.\n",
        "    \n",
        "    - Generating a dataset that identifies the user's dialect in a restaurant and responds in the same dialect.\n",
        "\n",
        "### Guides\n",
        "\n",
        "- Setup LiteLLM Proxy\n",
        "- Keep logs into a JSONL file\n",
        "- Setup the Pydantic Models\n",
        "- Setup the Prompt Template\n",
        "- Save generated data into a JSONL file\n",
        "- The generated data must follow this [popular format](https://github.com/hiyouga/LLaMA-Factory/blob/main/data/alpaca_en_demo.json)\n",
        "- Generate at least 50 samples.\n"
      ],
      "metadata": {
        "id": "JPPZLCNaP746"
      }
    }
  ]
}